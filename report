
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  

\IEEEoverridecommandlockouts                 

\overrideIEEEmargins

\usepackage{multibib}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{caption}
\usepackage{subcaption}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Exploring the effects of 3D space-time wavelets on an algorithm for Images Reconstruction from Brain Activity\\
}

\author{Francesco Guagliardo\\
Imperial College London,\\
UROP summer 2016\\
fg1014@ic.ac.uk\\
Supervisors: Dr. Anil Bharath, Stefania Garasto
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

My task for this UROP project was to implement the use of 3D space-time wavelets on an algorithm for images reconstruction from brain activity and see how this affected the quality of the reconstructions. The algorithm originally used 2D space wavelets to filter and then reconstruct the images. By adding the time dimension we wanted to make the algorithm closer to the way image processing happens in the brain. As a first step we decided to start by modelling our neurons as separable space time.\cite{thefather} We tested the new algorithm with two different sequences of images: a set of letters (16x16 pixels images like that in figure \ref{fig:letA}) and a set of images that represent the motion of a bar.
In both cases the new algorithm with the 3D space-time wavelets did not improve the quality of reconstructions, compared to the original algorithm. In this report we analyse why this may have been the case.

\end{abstract}



\section{INTRODUCTION}
The algorithm I was given access to was written by Stefania Garasto, a PhD candidate at the department of Bioengineering at Imperial College London. The program takes neural inputs from the visual cortex of mice, generated by showing a set of images to the mice, and converts them back into the original set of images. In order to do so, a mathematical model for every neuron involved in the processing of the image has to be created by the code. The model is created using the data obtained by filtering the set of images through Gabor filters. A lasso regression technique is used to create the models.
Gabor Wavelets (figure \ref{fig:wavelet}) are used as a mathematical approximation of the spacial receptive field of a simple cell in the visual cortex. \cite{thefather} When images are filtered using Gabor wavelets different features of the images are extrapolated depending on the size and orientation of the Wavelet. Hence by having many different sized wavelets with many different orientations we can extrapolate more information from the same image. A similar process happens in our visual cortex when we process the images we receive to our eyes. Receptive fields in the visual cortex have also a dependency on time. There are two types of space-time receptive fields: separable and non separable space time receptive fields. Therefore by adding the time dimension to our wavelet filters we would have a image processing system more similar to that in our brains.

A first step to include the time dimension to the algorithm was to treat all the receptive fields as separable. Separable receptive field can be modelled as a Gabor wavelet multiplied by a function of time (figure \ref{fig:DoE}). \cite{thefather}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{wavelet1.jpg}
    \caption{A Gabor wavelet used in the algorithm}
    \label{fig:wavelet}
\end{figure}


\section{RELATED WORK}

The research is inspired by previous attempts that were made using either fMRI data in humans or electrode recordings from the brain of cats. The developed algorithms showed some good results in terms of reconstructing the images, but fMRI and electrode recordings data have both advantages and disadvantages in terms of the information they can give. 
Stefania's research group, on the other hand, is using a recording technique that was recently developed, two-photons microscopy, to collect the responses of neurons in the brain of mice. This technique allows the imaging of the response of the brain down to the single cell and is capable of recording from a higher number of neurons than with electrodes. In this way we should be able to analyse different aspects of the neural response and to expand on the work that has been done previously, also by improving the algorithm to use.
The outcome of this research will help us understand how the brain processes visual information and could have future applications in the field of brain machine interface.

Previous works included attempts to recognise images from brain activity\cite{first}; reconstructions using a Bayesian framework\cite{second};
attempts of reconstructions of natural movies\cite{third};

\section{METHODS}

To give the algorithm the dependency on time that we wanted so that it could resemble more closely the way the receptive fields work in the visual cortex, we had to modify the set of wavelets used from to 2D to 3D, where the third dimension is the time dimension.
Conceptually adding the time dimension means that every image reconstruction is affected by what images have appeared up to 300ms before \cite{thefather} as we can see from the time function in Figure \ref{fig:DoE}. The function starts at zero, peaks at about 75ms, goes back to zero and has a negative peak at about 140ms, and then goes back to zero. In terms of visual perception this means that scenes that happened 75ms before what we are seeing now have the greatest impact on its perception and scenes that happened 140ms before have a negative impact. Scenes that happened before 140ms have gradually less impact on the perception of what we are seeing. 
This is how we perceive that an object is moving in our visual field.
The original algorithm used a total of 12 different 2D wavelets (figure \ref{fig:wavelet}), 3 different sizes and 4 orientation for each size. To make the wavelets 3D we had to multiply each wavelet by the time function in Figure \ref{fig:DoE}. Since the sampling rate used to gather the neural response is 30Hz, we had to sample down the time function to the same rate; the function used in the algorithm is therefore the one shown in figure \ref{fig:DoE33}. Figure \ref{fig:DoE33} contains 10 data points, therefore when multiplied with the wavelets, each wavelet will have 10 copies in the time dimension that change in amplitude following the time function. This means that in our algorithm, each reconstruction was affected by the 10 images that appeared before.
Adding one dimension to one of the fundamental part of the algorithm implied that many other parts had to be modified so that it could handle both 2D and 3D methods. 
Two parts in particular had to be modified to account for the change: the filtering part and the reconstruction part.

For the first we needed to add the convolution in time between the images (\(I\)) and the Gabor Wavelets (\(\phi\)) that originates the coefficients (\(\hat{f}\)) used to create the model. Mathematically, for the 2D case we used equation 1 to calculate the coefficients.

\begin{equation}
\hat{f}_k= \sum_{x,y}I(x,y)\phi_{k}(x,y)
\end{equation}

To account for the third dimention we had to add a convolution in time between the wavelets and the time function (\(g(t)\)). Equation 2 can be used for both 2D and 3D cases.

\begin{equation}
\hat{f}_k(t)= \sum_{x,y}I(x,y,t)\phi_{k}(x,y)\ast g(t)
\end{equation}

For the reconstruction the methos used is a linear sum with a basis wavelet (\(\phi\)) given by a combination of the quadrature pair. With 2D wavelets we just sum each quadrature pair that is going to take into account different parts of the image to reconstruct (equation 3).

\begin{equation}
\hat{S}(x,y)= \sum_{k}f_k\phi(x,y)
\end{equation}
Where \(\hat{S}(x,y)\) is the reconstructed frame.
But with 3D wavelets the sum takes into account also the 10 frames before the image to reconstruct, so each frame would be affected by what frame came before.
Hence equation 3 had to be modified to take this into account as shown in equation 4.

\begin{equation}
\hat{S}(x,y,t)= \sum_\tau\sum_{k}f_k(t+\tau)\phi(x,y)g(\tau)
\end{equation}

Each reconstruction \(\hat{S}(x,y,t)\) now also depends on time, is to say, to the 10 frames that appened before.


%   \begin{figure*}
%     \centering
%     \includegraphics[width=0.4\textwidth]{DoE-NonNormalised}
%     \caption{Time function, difference of exponentials (DoE)}
%     \label{fig:DoE}
% \end{figure*}
%  \begin{figure}
%     \centering
%     \includegraphics[width=0.4\textwidth]{DoE-NonNormalised33ms}
%     \caption{Time function, difference of exponentials (DoE) Sampled at 33ms}
%     \label{fig:DoE33}
% \end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{A}
    \caption{a nice plot}
    \label{fig:letA}
\end{figure}

\begin{figure*}
\centering
\begin{subfigure}[b]{8cm}
  \centering
  \includegraphics[width=1\linewidth]{DoE-NonNormalised}
  \caption{Time function}
  \label{fig:DoEn}
\end{subfigure}%
\begin{subfigure}[b]{8cm}
  \centering
  \includegraphics[width=1\linewidth]{DoE-NonNormalised33ms}
  \caption{Time function sampled at 33ms}
  \label{fig:DoE33}
\end{subfigure}
\caption{The time function that represent the temporal structure of a receptive filed}
\label{fig:DoE}
\end{figure*}

% \begin{figure}
% \centering
% \begin{minipage}{.5\textwidth}
%   \centering
%   \includegraphics[width=.4\linewidth]{DoE-NonNormalised}
%   \captionof{figure}{A figure}
%   \label{fig:test1}
% \end{minipage}%
% \begin{minipage}{.5\textwidth}
%   \centering
%   \includegraphics[width=.4\linewidth]{DoE-NonNormalised}
%   \captionof{figure}{Another figure}
%   \label{fig:test2}
% \end{minipage}
% \end{figure}

\subsection{Tables and images}



Example of table insertion in Table \ref{table_example}. Example of figure in Figure \ref{fig:letA}. When referencing tables and figures, add Table and Figure, respectively, before the reference..

\begin{table}[h]
\caption{An Example of a Table}
\label{table_example}
\begin{center}
\begin{tabular}{|c||c|}
\hline
One & Two\\
\hline
Three & Four\\
\hline
\end{tabular}
\end{center}
\end{table}


   
   
\section{RESULTS}

Did it work? How well? Provide some figures, and a table or two. How much time does it take?

PS. Overleaf supports git under the Share tab. Might be useful.

\section{CONCLUSIONS}

What are the strengths and shortcomings of your method? How well would it generalize to other game genres? How controllable is it, and what is the potential for using to adapt content to individual preferences? How would you develop it further, if you had time?



\addtolength{\textheight}{-12cm}  

%References:
\bibliographystyle{IEEEtran}
\bibliography{bibliography}




\end{document}
